{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM/BVdN5EKle6k3xZIdViL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefs-kiss/intro2ml/blob/main/LIN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient at a point"
      ],
      "metadata": {
        "id": "86wdkqQ9eOZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Partial derivatives of a function F(x,y)\n",
        "\n",
        "The code below computes the partial derivative of a function `F` with respect to one of its variables.\n",
        "\n",
        "To use it:\n",
        "\n",
        "* Update the function `F` to match the example from the slide deck.\n",
        "\n",
        "* Modify the `var` variable to specify which parameter you want to differentiate with respect to. This can be `x`, `y`, or any other variable used in your function.\n",
        "\n",
        "This will return the partial derivative of `F` with respect to the specified variable."
      ],
      "metadata": {
        "id": "ZWvz7K81YtOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "def derive(func, var):\n",
        "  F = sp.sympify(func)\n",
        "  var = sp.symbols(var)\n",
        "  return sp.diff(F, var)\n",
        "\n",
        "F = \"x**2 + 4*y*x\"\n",
        "var = \"y\"\n",
        "\n",
        "partial = \"dFd\" + var + \" :\"\n",
        "print(partial,derive(F, var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS8NGIApFuH1",
        "outputId": "4d9d22bb-44ef-42ae-f14f-a4e6bc47303b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dFdy : 4*x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient\n",
        "\n",
        "The gradient of a function is a vector, gives the direction in which the function increases the fastest near a point.\n",
        "\n",
        "We'd like to see some information about how our function `F` is chaning with respect to a particular point `(2,1)`.\n",
        "\n",
        "Use the `derive` function we created above to find the two partials. This will give us the gradient for `F(2,1)`."
      ],
      "metadata": {
        "id": "of__qHQ4b9Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "point = (2,1)\n",
        "x = point[0]\n",
        "y = point[1]\n",
        "\n",
        "partial_x = #your code here\n",
        "partial_y = #your code here\n",
        "\n",
        "partial_x, partial_y"
      ],
      "metadata": {
        "id": "zJHTb6mIccf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells us the rate of steepest increase from the point `(2,1)`, or rather, the direction to go in order to maximize our cost function.\n",
        "\n",
        "However, we'd like to minimize our cost function, so we're more interested in going in the direction of the greatest decrease. This is simply the negative gradient of `F(x,y)`. Run the code chunk below to get this value."
      ],
      "metadata": {
        "id": "qGlklIqObkwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-1*partial_x, -1*partial_y"
      ],
      "metadata": {
        "id": "GmP368dKd4wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've seen how to find a gradient at a particular point, we're going to see how this can be used in linear regression."
      ],
      "metadata": {
        "id": "7YvVynWrBKNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent\n"
      ],
      "metadata": {
        "id": "-wx4zoSneewa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General solution to gradient of cost function\n",
        "\n",
        "Let the cost function be denoted by `C`. The gradient of `C` with respect to the parameters is given by the vector `(dC/dB0, dC/dB1)`.\n",
        "\n",
        "Each component of the gradient tells us the rate of change of the cost function with respect to one of the parameters.\n",
        "\n",
        "In this gradient:\n",
        "\n",
        "* The first component `dC/dB0` represents how the cost function changes when `B0` increases by one unit.\n",
        "\n",
        "* The second component `dC/dB1`, represents how the cost function changes when `B1` increases by one unit.\n",
        "\n",
        "This gives us a general expression for the gradient that can be evaluated for any specific values of `B0` and `B1`."
      ],
      "metadata": {
        "id": "8Nc-IpEzJpSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `compute_gradient` function\n",
        "\n",
        "Below is a function `compute_gradient` that takes in some data (x_values, y_values) and returns the gradient of the cost function for linear regression."
      ],
      "metadata": {
        "id": "OzBFkRUXJCCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "\n",
        "def compute_gradient(x_values, y_values):\n",
        "    # define b0, b1, x, and y\n",
        "    b0, b1, x = sp.symbols('B0 B1 x')\n",
        "\n",
        "    # linear regression model\n",
        "    y_pred = b0 + b1 * x\n",
        "\n",
        "    # cost function (MSE)\n",
        "    C = sum((y_pred.subs(x, x_val) - y_val)**2 for x_val, y_val in zip(x_values, y_values)) / len(x_values)\n",
        "\n",
        "    # partial derivatives of the cost function with respect to b0 and b1\n",
        "    dC_db0 = sp.diff(C, b0)\n",
        "    dC_db1 = sp.diff(C, b1)\n",
        "\n",
        "    print(f\"The partial derivative of cost with respect to B0: {dC_db0}\")\n",
        "    print(f\"The partial derivative of cost with respect to B1: {dC_db1}\")\n",
        "\n",
        "    return dC_db0, dC_db1\n"
      ],
      "metadata": {
        "id": "9F8qAUGtYx1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's now take some points and store the x-values and y-values separately. We'll use the function above to find the gradient."
      ],
      "metadata": {
        "id": "FnrVgERMgDFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_values = [1, 3, 2, 3, 4]\n",
        "y_values = [3, 4, 3, 5, 5]\n",
        "\n",
        "partial_b0, partial_b1 = compute_gradient(x_values, y_values)"
      ],
      "metadata": {
        "id": "qaOC-mr7aTTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These partial derivatives give us a general form of any gradient for this particular set of data."
      ],
      "metadata": {
        "id": "m2mnPz40BWlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding explicit gradients and updating betas\n",
        "Once we have a generalized gradient calculated, we'll need to do two steps:\n",
        "* Step1: Plug in whatever the current `B0` and `B1` values are into the gradient above. This gives us an explicit gradient (an actual point in space)\n",
        "* Step2: Update `B0` and `B1` values by moving a small step away from the gradient from Step 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "Et0d4xaOgcaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `adjust_gradient` function\n",
        "\n",
        "Below is a function called `adjust_gradient` that takes in the partials from `compute_gradient`, our current beta values, and a learning rate, and returns the explicit gradient (some point) and our new beta values."
      ],
      "metadata": {
        "id": "hBa8a2PlJd6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_gradient(partial_b0, partial_b1, b0_init, b1_init, l):\n",
        "  b0, b1, x = sp.symbols('B0 B1 x')\n",
        "  # step1: substitute current values of b0 and b1 into the derivatives\n",
        "  partial_b0_val = partial_b0.subs({b0: b0_init, b1: b1_init, x: x_values[0], y: y_values[0]})\n",
        "  partial_b1_val = partial_b1.subs({b0: b0_init, b1: b1_init, x: x_values[0], y: y_values[0]})\n",
        "\n",
        "  # step2: Update b0 and b1, taking a small step in the opposite direction of the gradient\n",
        "  b0_val = b0_init - learning_rate * partial_b0_val\n",
        "  b1_val = b1_init - learning_rate * partial_b1_val\n",
        "\n",
        "  return partial_b0_val, partial_b1_val, b0_val, b1_val"
      ],
      "metadata": {
        "id": "yA6cyLpBaRty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see an example of using this function!"
      ],
      "metadata": {
        "id": "Vd9mXMxGhQvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b0 = 1\n",
        "b1 = 2\n",
        "learning_rate = 0.1\n",
        "partial_bo_updated, partial_b1_updated, b0_adj, b1_adj = adjust_gradient(partial_b0, partial_b1, b0,b1,learning_rate)\n",
        "\n",
        "print(f\"The gradient is ({partial_bo_updated}, {partial_b1_updated}) \\nThe adjusted betas are ({round(b0_adj,2)}, {round(b1_adj,2)})\")"
      ],
      "metadata": {
        "id": "4x8u4qrQealA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, now let's bring it all together!\n",
        "\n",
        "##Example data\n",
        "Let's see an example of finding a general gradient for a set of data, and then doing a few iterations of gradient descent. Below is some code to find the general form of a gradient, along with the first couple of iterations of gradient descent.\n",
        "\n",
        "Run the code chunk as is.\n",
        "\n",
        "We'd like to do a few more iterations. Update the for loop to do six iterations of this."
      ],
      "metadata": {
        "id": "3WpjW_wzhpr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "x_values = [1, 3, 2, 3, 4]\n",
        "y_values = [3, 4, 3, 5, 5]\n",
        "\n",
        "#starting values\n",
        "b0 = 1\n",
        "b1 = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "#computing general gradient\n",
        "print(\"General Gradient\")\n",
        "partial_b0, partial_b1 = compute_gradient(x_values, y_values)\n",
        "\n",
        "#a few iterations of gradient descent\n",
        "for i in range(2):\n",
        "  print(f\"\\nIteration {i+1}\")\n",
        "  partial_bo_updated, partial_b1_updated, b0, b1 = adjust_gradient(partial_b0, partial_b1, b0,b1,learning_rate)\n",
        "  print(f\"The gradient is ({round(partial_bo_updated,2)}, {round(partial_b1_updated,2)}) \\nThe adjusted betas are ({round(b0,2)}, {round(b1,2)})\")"
      ],
      "metadata": {
        "id": "xca9qfbJh066"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating the beta values with MSE\n",
        "\n",
        "Let's build a function `find_MSE` which takes in the beta values `B0`, `B1`, and our data `(x_values, y_values)` and computes the MSE.\n",
        "\n",
        "* Add code to use our model to predict y-values given our feature set X. The format is `model.predict(features)`\n",
        "* Add code to compute the mse between those predicted y-values and the actual y-values. Hint: what function do we import at the top of the code."
      ],
      "metadata": {
        "id": "aC4gwu0OjaOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def find_MSE(B0, B1, x, y):\n",
        "  #reshaping data to be arrays\n",
        "  x_train = np.array(x).reshape(-1, 1)\n",
        "  y_train = np.array(y)\n",
        "  #building lin reg model with betas from gradient descent\n",
        "  lin_reg = LinearRegression()\n",
        "  lin_reg.intercept_ = B0\n",
        "  lin_reg.coef_ = np.array([B1])\n",
        "  #predict y values given the model above\n",
        "  y_pred = #your code here\n",
        "  #compute mse\n",
        "  mse = #your code here\n",
        "  return mse\n"
      ],
      "metadata": {
        "id": "Om91CLoBjnx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "kzpGTTx3KC-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert(find_MSE(0.56, 0.60, x_values, y_values)==3.7488)"
      ],
      "metadata": {
        "id": "HiotZS-0CwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example data, now with MSE\n",
        "\n",
        "Let's add this to our code from above.\n",
        "* Copy and paste the gradient descent code from the example data section (where we had the for loop) to the code chunk below.\n",
        "\n",
        "* Add a new line to the body of the for loop to compute and print the mse for the given beta values and data."
      ],
      "metadata": {
        "id": "MdwlfEp_Dxlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "k002DZRGDxGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How long to iterate?\n",
        "Now, we'd like to run this a bit until we find our ideal beta values. Change the for loop to run 150 times.\n",
        "\n",
        "* What iteration do we find our ideal beta values? Hint: find the iteration where the beta values stop changing.\n",
        "\n",
        "* What do you notice about the gradient? Why is this happening?\n",
        "\n",
        "* What are the ideal B0 and B1 values? What is the equation of the regression line for this data?\n",
        "\n",
        "* Update the B0 and B1 variables in the plot below with these values. Does this look like a good fit?"
      ],
      "metadata": {
        "id": "fVG4ypHZIj8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Plot the points\n",
        "plt.scatter(x_values, y_values, color='black', marker='o')\n",
        "plt.grid(True)\n",
        "\n",
        "#equation of the line\n",
        "x_line = np.linspace(min(x_values) - 1, max(x_values) + 1, 100)\n",
        "B1 = 0 #change slope\n",
        "B0 = 0 #change intercept\n",
        "y_line = B1 * x_line + B0\n",
        "plt.plot(x_line, y_line, color='tab:olive', linewidth=2)"
      ],
      "metadata": {
        "id": "_NtmYwXKWdJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Learning rate\n",
        "\n",
        "This is the size of step we take towards the direction of steepest descent. Too big of a step, we’ll overstep where our target is (similar to divergence). Too small, and it’ll take forever to get where we’re trying to go (small convergence). We're looking for the ideal learning rate to get us to the minima as efficiently as possible.\n",
        "\n",
        "\n",
        "Now let's see what happens when we change the learning rate.\n",
        "* update the loop so that it only runs 5 times.\n",
        "* update the learning rate to 0.15. What is happening to the MSE?\n",
        "* update learning rate to 0.3. What is happening to MSE?\n",
        "* update learning rate to 0.05. What is happening to MSE?\n",
        "* update learning rate to 0.001. What is happening to MSE?\n",
        "* play around with the learning rate until the last iteration MSE is as low as possible. What is this value?"
      ],
      "metadata": {
        "id": "MtAfw5XRKr7p"
      }
    }
  ]
}